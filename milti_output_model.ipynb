{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of unet_auto_multi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-r6kZLKiAxf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "15243550-ada1-40ca-ad86-61c44f228ec3"
      },
      "source": [
        "!wget -O 'dataset' 'https://storage.googleapis.com/kaggle-data-sets/111880%2F269359%2Fbundle%2Farchive.zip?GoogleAccessId=gcp-kaggle-com@kaggle-161607.iam.gserviceaccount.com&Expires=1593864510&Signature=VI7UzqQr9WUFt6tPspjaciQ0rvWhOnzBd02wN02BcSTRafyjSTW6JtV5GOy9A8Ro1llbalKOvPt%2FfIOFdc4Kus6GNuJ%2FtGWMlMT%2BUKYjrvVOMfG3rFSk0rqyfz5oD4bkCYNZUfdK2xjsK4LGisHH7JtK4Pey2wgREpIqCXcHq3H0etyzwalxMbY2zogpk%2Bq5vXzOFZRj%2BX1BKbFz7%2Bpb0vqK6eWVqPyhZD4VUQQ%2FNocqVuX4BQpSFiyx6X0tEyx8cXSaVIWrKwzRv9CsVV4dTrIp775IjkFj0EIKQwox5CeLc9fE3zvx%2BccNodZ0aW1lips%2BHmtJu6n3PovPSfIfJw%3D%3D'\n",
        "\n",
        "# !wget -O 'dataset' 'https://storage.googleapis.com/kaggle-data-sets/137362%2F325302%2Fbundle%2Farchive.zip?GoogleAccessId=gcp-kaggle-com@kaggle-161607.iam.gserviceaccount.com&Expires=1593864065&Signature=BB9LoFIi0IcMyKUVokgqtCawp0iBKiId0EM6O6iABczGy5E9Vru7V5AMTEfYz%2Bfhks6MuChZVk1qk%2BD%2BnNaZemqLHpsUWMATh2a6GWE3lcx00kWXv160euEFfjCTUvEJl%2F7KEI%2FEQAekoV6Kdvwen57%2Fp5UFS%2BYUVA75DftxxLlJwyV4O9otAH1peYIlNIWWcmD%2B8CxfbaHEiLbnF5U9me7mkFYP3c2GqXEshbRWSps0EezuJCgSaCD%2B3ePsMKNfd9jdykdSKrX8gwNl%2FTw6bxKyH%2F%2By4Dm%2BmMD7m4bqlRkthyGMa5lNTnyIwbFdReWnIGKY98%2BOaaf0IZb317%2FJBw%3D%3D'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-01 13:35:19--  https://storage.googleapis.com/kaggle-data-sets/111880%2F269359%2Fbundle%2Farchive.zip?GoogleAccessId=gcp-kaggle-com@kaggle-161607.iam.gserviceaccount.com&Expires=1593864510&Signature=VI7UzqQr9WUFt6tPspjaciQ0rvWhOnzBd02wN02BcSTRafyjSTW6JtV5GOy9A8Ro1llbalKOvPt%2FfIOFdc4Kus6GNuJ%2FtGWMlMT%2BUKYjrvVOMfG3rFSk0rqyfz5oD4bkCYNZUfdK2xjsK4LGisHH7JtK4Pey2wgREpIqCXcHq3H0etyzwalxMbY2zogpk%2Bq5vXzOFZRj%2BX1BKbFz7%2Bpb0vqK6eWVqPyhZD4VUQQ%2FNocqVuX4BQpSFiyx6X0tEyx8cXSaVIWrKwzRv9CsVV4dTrIp775IjkFj0EIKQwox5CeLc9fE3zvx%2BccNodZ0aW1lips%2BHmtJu6n3PovPSfIfJw%3D%3D\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.133.128, 108.177.15.128, 173.194.76.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.133.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 363152213 (346M) [application/zip]\n",
            "Saving to: ‘dataset’\n",
            "\n",
            "dataset             100%[===================>] 346.33M  50.8MB/s    in 6.8s    \n",
            "\n",
            "2020-07-01 13:35:27 (50.8 MB/s) - ‘dataset’ saved [363152213/363152213]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR-QO6x8kQxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_zip = 'dataset'\n",
        "cpy_zip = 'dataset_folder/' \n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(src_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(cpy_zip)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA-Eq6E_kd7D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "658a9263-383e-4278-c39e-1c80d84156f2"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import random\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYA7_lcNknar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_lab_dict(path_dataset):\n",
        "    folders = os.listdir(path_dataset)\n",
        "    folders = np.sort(folders)  \n",
        "    labs = list(range(0,len(folders)))\n",
        "    lab_dict = dict(zip(folders, labs))\n",
        "    \n",
        "    return lab_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzGLIqRe74yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.keras.layers import BatchNormalization, Flatten, Lambda\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Conv2DTranspose, Conv2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "CHANNELS = 3\n",
        "#SCALED_Y,SCALED_X = (128,128)\n",
        "\n",
        "\n",
        "def build_encoder(in_img, filters = 32):\n",
        "    def conv2d(layer_input, filters, f_size=4, bn=True):\n",
        "        d = Conv2D(filters, kernel_size=f_size,\n",
        "                   strides=(2,2), padding='same')(layer_input)\n",
        "        \n",
        "        if bn:\n",
        "            d = BatchNormalization(momentum=0.8)(d)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "\n",
        "        return d\n",
        "    \n",
        "    #d0 = Input(shape=img_shape)    \n",
        "    d0_b = Conv2D(filters, kernel_size=4,strides=1, padding='same', activation='relu')(in_img)\n",
        "  \n",
        "    # Downsampling: 7 x stride of 2 --> x1/128 downsampling\n",
        "    d1 = conv2d(d0_b, filters, bn=False)\n",
        "    d2 = conv2d(d1, filters * 2)\n",
        "    d3 = conv2d(d2, filters * 4)\n",
        "    d4 = conv2d(d3, filters * 8)\n",
        "    d5 = conv2d(d4, filters * 8)\n",
        "    d6 = conv2d(d5, filters * 8)\n",
        "    d7 = conv2d(d6, filters * 8)\n",
        "    \n",
        "    return d7\n",
        "\n",
        "def build_decoder(input_features,channels=CHANNELS,filters = 32, output_activation = 'sigmoid'):\n",
        "    \n",
        "    def deconv2d(layer_input, filters, f_size=4, dropout_rate=0):\n",
        "        \n",
        "        u = Conv2DTranspose(filters, kernel_size=f_size, strides=(2,2),\n",
        "                            padding='same', activation='linear')(layer_input)\n",
        "        u = Conv2D(filters, kernel_size=f_size, strides=1,\n",
        "                   padding='same',activation='relu')(u)\n",
        "\n",
        "\n",
        "        u = BatchNormalization(momentum=0.8)(u)\n",
        "        if dropout_rate:\n",
        "            u = Dropout(dropout_rate)(u)\n",
        "        \n",
        "        return u\n",
        "    \n",
        "    #input_features = Input(shape=feat_shape) \n",
        "    # Upsampling: 6 x stride of 2 --> x64 upsampling \n",
        "    u1 = deconv2d(input_features ,filters * 8)\n",
        "    u2 = deconv2d(u1, filters * 8)\n",
        "    u3 = deconv2d(u2, filters * 8)\n",
        "    u4 = deconv2d(u3, filters * 4)\n",
        "    u5 = deconv2d(u4, filters * 2)\n",
        "    u6 = deconv2d(u5, filters)\n",
        "    \n",
        "    u7 = deconv2d(u6, filters)\n",
        "    \n",
        "    # added conv layers after the deconvs to avoid the pixelated outputs    \n",
        "    output_img = Conv2D(channels, kernel_size=4,\n",
        "                        strides=1, padding='same',\n",
        "                        activation=output_activation)(u7)\n",
        "    \n",
        "    return output_img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShrR5Wgew90O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_paths(path_dataset):\n",
        "    all_paths = glob(path_dataset+'/'+'*'+'/'+'*')\n",
        "    return all_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wagSktKitScx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_datagen(path_dataset, lab_dict, batch_size=64, out_shape=(128,128,3), mode='auto'):\n",
        "    \n",
        "    while True: \n",
        "        all_paths = get_paths(path_dataset)\n",
        "        selected_paths = random.sample(all_paths, batch_size)\n",
        "        \n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "        for i in range(0,len(selected_paths)):\n",
        "            img = cv2.imread(selected_paths[i])\n",
        "            img = cv2.resize(img,(out_shape[0],out_shape[1]))\n",
        "\n",
        "            img_category = selected_paths[i].split('/')[-2]\n",
        "            lab = lab_dict[img_category]\n",
        "            lab = to_categorical(lab, num_classes=6)\n",
        "            # print(img_category)\n",
        "            # print(lab)\n",
        "            batch_images.append(img)\n",
        "            batch_labels.append(lab)\n",
        "        if mode=='auto':\n",
        "            yield np.array(batch_images)/255, np.array(batch_images)/255\n",
        "        elif mode=='classi':\n",
        "            yield np.array(batch_images)/255, np.array(batch_labels)\n",
        "        else:\n",
        "            yield np.array(batch_images)/255, (np.array(batch_images)/255,np.array(batch_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J99wYgiNTCrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_multitask_model(input_shape):\n",
        "    input_img = Input(shape = input_shape)\n",
        "    enc = build_encoder(input_img)\n",
        "\n",
        "    x = Flatten()(enc)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dense(6,activation='softmax')(x)\n",
        "\n",
        "    y = build_decoder(enc)\n",
        "\n",
        "    multi_model = Model(input_img, [y,x])\n",
        "\n",
        "    return multi_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwfkYXHl2JJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_model(model_name='my_model', path_ckpt='checkpoint/', input_shape = (128,128,3),mode='auto'):\n",
        "    complete_path = path_ckpt+model_name+'.h5'\n",
        "\n",
        "    if mode=='auto':    \n",
        "        input_img = Input(shape = input_shape)\n",
        "        model = Model(input_img, build_decoder(build_encoder(input_img)))\n",
        "    else:\n",
        "        model = build_multitask_model(input_shape=input_shape)\n",
        "        \n",
        "    if not os.path.isdir(path_ckpt):\n",
        "        os.mkdir(path_ckpt)\n",
        "\n",
        "    names = os.listdir(path_ckpt)\n",
        "    if complete_path.split('/')[-1] in names:\n",
        "        print('Loading checkpoint...')\n",
        "        model.load_weights(complete_path)\n",
        "    else:\n",
        "        print('Training from scratch')\n",
        "\n",
        "    return model, complete_path\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePfTlXwJ4nVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(path_dataset, batch_size=64, epochs=2, img_shape=(128,128,3), mode='auto'):\n",
        "    model, checkpoint_path = init_model(input_shape=img_shape, mode=mode)\n",
        "    \n",
        "    checkpoint=ModelCheckpoint(filepath=checkpoint_path,monitor='loss',save_best_only=True,\n",
        "                           save_weights_only=False,verbose=0,mode=\"auto\")\n",
        "    \n",
        "    # checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='loss', save_best_only=False, \n",
        "    #                              save_weights_only=False, mode='auto', period=2)\n",
        "\n",
        "    callbacks=[checkpoint]\n",
        "    print('learning rate = ',learn_rate)\n",
        "    opt = tf.keras.optimizers.Adam(lr = learn_rate)\n",
        "    if mode=='auto': \n",
        "        model.compile(optimizer=opt,loss=\"mse\")\n",
        "    else:\n",
        "        model.compile(optimizer=opt,loss=[\"mse\", \"categorical_crossentropy\"])\n",
        "    \n",
        "    lab_dict = make_lab_dict(path_dataset)\n",
        "    print(lab_dict)\n",
        "\n",
        "    data_size = len(get_paths(path_dataset))\n",
        "\n",
        "    train_gen = my_datagen(path_dataset, lab_dict, batch_size=batch_size, out_shape=img_shape, mode=mode)\n",
        "    model.fit_generator(train_gen, epochs=epochs, callbacks = callbacks,steps_per_epoch=int(data_size/batch_size))\n",
        "\n",
        "    # save trained model\n",
        "    #model.save('checkpoint/' + 'trained_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h5wVcUz9M0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global learn_rate\n",
        "learn_rate = 0.0001\n",
        "\n",
        "batch_size = 256\n",
        "input_shape = (128,128,3)\n",
        "path_dataset = 'dataset_folder/seg_train/seg_train/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp5jDWcQ902P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "713888cc-f4e0-4dd8-c7fc-035ee6f59fa4"
      },
      "source": [
        "train_model(path_dataset, batch_size, 300, input_shape, mode='multi')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training from scratch\n",
            "learning rate =  0.0001\n",
            "{'buildings': 0, 'forest': 1, 'glacier': 2, 'mountain': 3, 'sea': 4, 'street': 5}\n",
            "WARNING:tensorflow:From <ipython-input-10-9047e798cdcd>:24: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/300\n",
            "36/54 [===================>..........] - ETA: 30s - loss: 1.1555 - conv2d_15_loss: 0.0828 - dense_1_loss: 1.0727"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aYGw58uY6nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}